{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae260752-bd0c-4bd2-97aa-e2a2a3d6c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "# LlamaIndex will download embeddings models as needed\n",
    "# Set llamaindex cache dir to ../cache dir here (Default is system tmp)\n",
    "# This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath('./'), 'cache')\n",
    "\n",
    "# Load settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "ATLAS_URI = config.get('ATLAS_URI')\n",
    "GROQ_API = config.get('GROQ_API')\n",
    "\n",
    "dataset = load_dataset(\"fronkongames/steam-games-dataset\")\n",
    "\n",
    "# To disable GPU and experiment, uncomment the following line\n",
    "# Normally, you would want to use GPU, if one is available\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "print (\"using CUDA/GPU: \", torch.cuda.is_available())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(\"device \", i , torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd56010-b0e2-494d-aefc-8d73f297aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "datasets = pd.DataFrame(dataset['train'])\n",
    "\n",
    "datasets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ad5f9-c22a-46e4-8b74-4e1d3b3ce17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data point where About the game is missing\n",
    "dataset_games = datasets.dropna(subset=['About the game', 'Metacritic url'])\n",
    "print(\"\\nNumber of missing values in each column after removal:\")\n",
    "print(dataset_games.isnull().sum())\n",
    "\n",
    "# Remove the unecessary columns\n",
    "dataset_games = dataset_games.drop(columns=['Screenshots', 'Movies', 'Notes', 'Average playtime forever', 'Average playtime two weeks', 'Median playtime forever', 'Median playtime two weeks', 'Score rank', 'Website', 'Support url', 'Full audio languages'])\n",
    "\n",
    "# Remove data with Recomendations score = 0\n",
    "# dataset_games = dataset_games.query(\"`Recommendations` != 0\")\n",
    "\n",
    "length_dataset = len(dataset_games)\n",
    "print(length_dataset)\n",
    "\n",
    "dataset_games.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d2182-ccc0-449f-aad9-50d806170528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and classes\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Initialize the Groq Api object with your API key\n",
    "llm = Groq(model=\"mixtral-8x7b-32768\", api_key=GROQ_API)\n",
    "\n",
    "# Initialize the HuggingFaceEmbedding object with the specified model and device\n",
    "embed_model = HuggingFaceEmbedding(model_name='Alibaba-NLP/gte-base-en-v1.5', device='cuda', trust_remote_code=True)\n",
    "\n",
    "# Set the LLM and embed_model in the Settings for further usage\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e18a3-cfdf-485c-b8a4-83ab5655a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "# Convert the DataFrame to a JSON string representation\n",
    "documents_json = dataset_games.to_json(orient='records')\n",
    "\n",
    "# Load the JSON string into a Python list of dictionaries\n",
    "documents_list = json.loads(documents_json)\n",
    "\n",
    "llama_documents = []\n",
    "\n",
    "# Helper function to convert any type to a string\n",
    "def safe_str(value):\n",
    "    if isinstance(value, list):\n",
    "        return ', '.join(map(str, value))\n",
    "    return str(value)\n",
    "\n",
    "for document in documents_list:\n",
    "\n",
    "    # Value for metadata must be one of (str, int, float, None)\n",
    "    document['Tags'] = json.dumps(document['Tags'])\n",
    "    document['Supported languages'] = json.dumps(document['Supported languages'])\n",
    "    document['Reviews'] = json.dumps(document['Reviews'])\n",
    "    document['Achievements'] = json.dumps(document['Achievements'])\n",
    "\n",
    "    # Create a Document object with the text and excluded metadata for llm and embedding models\n",
    "    llama_document = Document(\n",
    "        text=safe_str(document['About the game']) + '.\\n' + \n",
    "            \"Price \" + safe_str(document['Price']) + '.\\n' + \n",
    "            \"Developers \" + safe_str(document['Developers']) + '.\\n' + \n",
    "            \"Publishers \" + safe_str(document['Publishers']) + '.\\n' + \n",
    "            \"Recommendations \" + safe_str(document['Recommendations']) + '.\\n' + \n",
    "            \"Metacritic score \" + safe_str(document['Metacritic score']) + '.\\n' +\n",
    "            \"Categories \" + safe_str(document['Categories']) + '.\\n' + \n",
    "            \"Genres \" + safe_str(document['Genres']) + '.',\n",
    "        metadata=document,\n",
    "        excluded_llm_metadata_keys=['About the game', 'Price', 'Developers', 'Publishers', 'Recommendations', 'Metacritic score', 'Categories', 'Genres'],\n",
    "        excluded_embed_metadata_keys=['About the game', 'Price', 'Developers', 'Publishers', 'Recommendations', 'Metacritic score', 'Categories', 'Genres', 'Header image'],\n",
    "        metadata_template=\"{key}=>{value}\",\n",
    "        text_template=\"Metadata: {metadata_str}\\n--------\\nContent: {content}\",\n",
    "    )\n",
    "\n",
    "    llama_documents.append(llama_document)\n",
    "\n",
    "# Observing an example of what the llm and Embedding model receive as input\n",
    "print(\n",
    "    \"\\nThe LLM sees this: \\n\",\n",
    "    llama_documents[0].get_content(metadata_mode=MetadataMode.LLM),\n",
    ")\n",
    "print(\n",
    "    \"\\nThe Embedding model sees this: \\n\",\n",
    "    llama_documents[0].get_content(metadata_mode=MetadataMode.EMBED),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e7770-d5d8-4932-a39f-4ce01cddabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SentenceSplitter class from the node_parser module\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Initialize a SentenceSplitter object\n",
    "parser = SentenceSplitter()\n",
    "\n",
    "# Use the SentenceSplitter to parse nodes from the llama_documents\n",
    "nodes = parser.get_nodes_from_documents(llama_documents)\n",
    "\n",
    "# Iterate through each node\n",
    "for node in nodes:\n",
    "    # Get the content of the node with metadata included\n",
    "    content_with_metadata = node.get_content(metadata_mode=\"all\")\n",
    "    \n",
    "    # Get the text embedding for the node content using the embed_model defined in the Settings\n",
    "    node_embedding = Settings.embed_model.get_text_embedding(content_with_metadata)\n",
    "    \n",
    "    # Assign the calculated embedding to the node\n",
    "    node.embedding = node_embedding\n",
    "\n",
    "print(\"Total nodes data:\", len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b1fe5-a268-4283-bb3c-59b49e32a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mongo_client(mongo_uri):\n",
    "    \"\"\"Establish connection to the MongoDB.\"\"\"\n",
    "    try:\n",
    "        client = MongoClient(mongo_uri)  # Establish connection to MongoDB using the provided URI.\n",
    "        print(\"Connection to MongoDB successful\")  # Print success message if connection is established.\n",
    "        return client\n",
    "    except Error as e:\n",
    "        print(f\"Connection failed: {e}\")  # Print error message if connection fails.\n",
    "        return None\n",
    "\n",
    "mongo_uri = config.get('ATLAS_URI')  # Retrieve MongoDB URI from environment variables.\n",
    "if not mongo_uri:\n",
    "    print(\"MONGO_URI not set in environment variables\")  # Print a warning if MongoDB URI is not set.\n",
    "\n",
    "mongo_client = get_mongo_client(mongo_uri)  # Establish MongoDB client connection.\n",
    "\n",
    "DB_NAME = \"steam_games2\"  # Name of the database to use.\n",
    "COLLECTION_NAME = \"embedded_games2\"  # Name of the collection to use within the database.\n",
    "\n",
    "db = mongo_client[DB_NAME]  # Select the specified database.\n",
    "collection = db[COLLECTION_NAME]  # Select the specified collection within the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc866acd-5a00-4aa8-9679-c742bc46680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure we are working with a fresh collection\n",
    "# Delete any existing records in the collection\n",
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da04af1-45a0-4162-b59b-4ee849082de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the MongoDBAtlasVectorSearch class for vector search functionality.\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "import threading\n",
    "\n",
    "# Initialize a MongoDBAtlasVectorSearch instance with the MongoDB client, database name, collection name, and index name.\n",
    "# This object will be used for vector search operations on the specified MongoDB collection.\n",
    "vector_store = MongoDBAtlasVectorSearch(mongo_client, db_name=DB_NAME, collection_name=COLLECTION_NAME, index_name=\"games_index2\")\n",
    "\n",
    "# Function to add nodes to MongoDB collection with threading\n",
    "def add_nodes_with_threading(vector_store, nodes):\n",
    "    def add_nodes_thread():\n",
    "        for node in nodes:\n",
    "            vector_store.add([node])\n",
    "\n",
    "    # Create new thread to add nodes\n",
    "    thread = threading.Thread(target=add_nodes_thread)\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "\n",
    "# Calling function to adding nodes with threading\n",
    "add_nodes_with_threading(vector_store, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cffa7-424f-4698-aad1-8f3248bd0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex  # Import the VectorStoreIndex class for indexing vector stores.\n",
    "\n",
    "# Initialize a MongoDBAtlasVectorSearch instance with the MongoDB client, database name, collection name, and index name.\n",
    "# This object will be used for vector search operations on the specified MongoDB collection.\n",
    "vector_store = MongoDBAtlasVectorSearch(mongo_client, db_name=DB_NAME, collection_name=COLLECTION_NAME, index_name=\"games_index2\")\n",
    "\n",
    "# Create an index using the VectorStoreIndex class, initializing it with the provided vector store.\n",
    "# This index will enable efficient search operations on the vectors stored in the MongoDB collection.\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515cc152-5aa1-434c-81b2-dffe9f717446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if our RAG system is working\n",
    "\n",
    "# Importing Markdown for displaying formatted text in IPython notebooks.\n",
    "from IPython.display import Markdown\n",
    "# Importing utility function for displaying query responses in notebooks.\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "# Create a query engine from the index for querying vector data efficiently.\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Define the query string to search for relevant games.\n",
    "query = \"Is Red Dead Redemption 2 a good game?\"\n",
    "\n",
    "# Perform the query using the query engine to retrieve relevant results.\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Display the response as bold text using Markdown.\n",
    "display(Markdown(f\"<b>{response}</b>\\n\"))\n",
    "\n",
    "# Display the response with source information using the provided utility function.\n",
    "display_response(response, show_source=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-1",
   "language": "python",
   "name": "atlas-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
